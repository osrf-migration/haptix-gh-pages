{"priority": "major", "kind": "enhancement", "repository": {"links": {"self": {"href": "data/repositories/osrf/handsim.json"}, "html": {"href": "#!/osrf/handsim"}, "avatar": {"href": "data/bytebucket.org/ravatar/{9c7d89af-6eb6-40b1-9f69-da1ae9d129ce}ts=c_plus_plus"}}, "type": "repository", "name": "handsim", "full_name": "osrf/handsim", "uuid": "{9c7d89af-6eb6-40b1-9f69-da1ae9d129ce}"}, "links": {"attachments": {"href": "data/repositories/osrf/handsim/issues/85/attachments_page=1.json"}, "self": {"href": "data/repositories/osrf/handsim/issues/85.json"}, "watch": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/handsim/issues/85/watch"}, "comments": {"href": "data/repositories/osrf/handsim/issues/85/comments_page=1.json"}, "html": {"href": "#!/osrf/handsim/issues/85/support-video-showing-experiment-where-we"}, "vote": {"href": "https://api.bitbucket.org/2.0/repositories/osrf/handsim/issues/85/vote"}}, "reporter": {"display_name": "David Kluger", "uuid": "{0f991d57-5bb5-4dc7-bee0-56547fac3a17}", "links": {"self": {"href": "https://api.bitbucket.org/2.0/users/%7B0f991d57-5bb5-4dc7-bee0-56547fac3a17%7D"}, "html": {"href": "https://bitbucket.org/%7B0f991d57-5bb5-4dc7-bee0-56547fac3a17%7D/"}, "avatar": {"href": "https://bitbucket.org/account/DKluger/avatar/"}}, "nickname": "DKluger", "type": "user", "account_id": null}, "title": "Support video showing experiment where we use phantom objects", "component": null, "votes": 0, "watches": 2, "content": {"raw": "I apologize if this is not the correct place for me to put this, but I couldn't find a better option on the handsim bitbucket page.\r\n\r\n\r\nFor the phantom object function, I have attached a video of one of our experimental sessions using our old virtual prosthetic limb to show you what we are talking about. In this experiment, the volunteer is not looking at the screen (but he does look in other trials), and he is instructed to flex the virtual fingers to a close or far target. Our motor decode interprets his finger location, and stimulation is applied invoking a sensory perception when his fingers are within the targets. When all fingers are in the correct location, the targets go from red to green. An audible beep is heard when all fingers are in the correct location for ~1 second. The volunteer then indicates whether he thinks the targets are close or far. We would like to recreate this experiment in the new VREs.\r\n\r\nNote that the targets do not impede the motion of the robot's fingers at all (thus why we want the \"phantom\" objects). If implementation for the overlap function is not feasible, there is a workaround we can use. This workaround is using an hx_read_sensors() call in an if statement to find when the motors are in a small range of target positions, and a target sphere could be placed roughly in that range of positions. That sphere would have collisions turned off, but would change color when the finger motor is in the range of target positions. This workaround does introduce some error between where the target is located in virtual space and what finger motor positions cause the fingertip to be in the target. This is why it would be nice, but not 100% necessary, to have the ability to precisely determine when the robot's fingers are overlapping with a target.", "markup": "markdown", "html": "<p>I apologize if this is not the correct place for me to put this, but I couldn't find a better option on the handsim bitbucket page.</p>\n<p>For the phantom object function, I have attached a video of one of our experimental sessions using our old virtual prosthetic limb to show you what we are talking about. In this experiment, the volunteer is not looking at the screen (but he does look in other trials), and he is instructed to flex the virtual fingers to a close or far target. Our motor decode interprets his finger location, and stimulation is applied invoking a sensory perception when his fingers are within the targets. When all fingers are in the correct location, the targets go from red to green. An audible beep is heard when all fingers are in the correct location for ~1 second. The volunteer then indicates whether he thinks the targets are close or far. We would like to recreate this experiment in the new VREs.</p>\n<p>Note that the targets do not impede the motion of the robot's fingers at all (thus why we want the \"phantom\" objects). If implementation for the overlap function is not feasible, there is a workaround we can use. This workaround is using an hx_read_sensors() call in an if statement to find when the motors are in a small range of target positions, and a target sphere could be placed roughly in that range of positions. That sphere would have collisions turned off, but would change color when the finger motor is in the range of target positions. This workaround does introduce some error between where the target is located in virtual space and what finger motor positions cause the fingertip to be in the target. This is why it would be nice, but not 100% necessary, to have the ability to precisely determine when the robot's fingers are overlapping with a target.</p>", "type": "rendered"}, "assignee": null, "state": "invalid", "version": null, "edited_on": null, "created_on": "2015-05-04T19:13:54.570064+00:00", "milestone": null, "updated_on": "2015-05-06T20:29:47.217447+00:00", "type": "issue", "id": 85}